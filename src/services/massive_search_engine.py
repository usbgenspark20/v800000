
"""
Massive Search Engine - Sistema de Busca Massiva
Coleta dados at√© atingir 300KB m√≠nimo salvando em RES_BUSCA_[PRODUTO].json
"""

import os
import json
import logging
import asyncio
from typing import Dict, List, Any, Optional
from datetime import datetime
import sys
import time

# Importa√ß√µes de servi√ßos
from services.alibaba_websailor import alibaba_websailor
from services.real_search_orchestrator import RealSearchOrchestrator
from services.auto_save_manager import auto_save_manager # Importa√ß√£o movida para o topo

# Adicionar o diret√≥rio src ao path
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

logger = logging.getLogger(__name__)

class MassiveSearchEngine:
    """Sistema de busca massiva com m√∫ltiplas APIs e rota√ß√£o"""

    def __init__(self):
        self.websailor = alibaba_websailor  # ALIBABA WebSailor
        self.real_search = RealSearchOrchestrator()  # Real Search Orchestrator
        self.auto_save_manager = auto_save_manager # Inst√¢ncia do auto_save_manager

        self.min_size_kb = int(os.getenv('MIN_JSON_SIZE_KB', '500'))
        self.min_size_bytes = self.min_size_kb * 1024
        self.data_dir = os.getenv('DATA_DIR', 'analyses_data')

        os.makedirs(self.data_dir, exist_ok=True)

        logger.info(f"üîç Massive Search Engine inicializado - M√≠nimo: {self.min_size_kb}KB")

    async def execute_massive_search(self, produto: str, publico_alvo: str, session_id: str, **kwargs) -> Dict[str, Any]:
        """
        Executa busca massiva at√© atingir 300KB m√≠nimo
        Salva em RES_BUSCA_[PRODUTO].json

        Aceita **kwargs para evitar erros com argumentos inesperados (ex: 'query').
        """
        # Logar argumentos inesperados para depura√ß√£o
        if kwargs:
            logger.warning(f"‚ö†Ô∏è Argumentos inesperados recebidos e ignorados: {list(kwargs.keys())}")

        try:
            logger.info(f"üöÄ INICIANDO BUSCA MASSIVA: {produto}")

            # Arquivo de resultado
            produto_clean = produto.replace(' ', '_').replace('/', '_')
            resultado_file = os.path.join(self.data_dir, f"RES_BUSCA_{produto_clean.upper()}.json")

            # Estrutura de dados massiva
            massive_data = {
                'produto': produto,
                'publico_alvo': publico_alvo,
                'session_id': session_id,
                'timestamp_inicio': datetime.now().isoformat(),
                'busca_massiva': {
                    'alibaba_websailor_results': [],  # ALIBABA WebSailor
                    'real_search_orchestrator_results': []  # Real Search Orchestrator
                },
                'viral_content': [],
                'marketing_insights': [],
                'competitor_analysis': [],
                'social_media_data': [],
                'content_analysis': [],
                'trend_analysis': [],
                'metadata': {
                    'total_searches': 0,
                    'apis_used': [],
                    'size_kb': 0,
                    'target_size_kb': self.min_size_kb
                }
            }

            # Queries de busca massiva
            search_queries = self._generate_search_queries(produto, publico_alvo)

            logger.info(f"üìã {len(search_queries)} queries geradas para busca massiva")

            # Executar buscas com limite fixo para evitar loop infinito
            current_size = 0
            search_count = 0
            max_searches = min(len(search_queries), 10)  # M√°ximo 10 buscas para evitar loop

            for query in search_queries[:max_searches]:  # Processa apenas as primeiras queries
                search_count += 1
                logger.info(f"üîç Busca {search_count}: {query[:50]}...")

                # ALIBABA WebSailor - PRINCIPAL
                try:
                    websailor_result = await self._search_alibaba_websailor(query, session_id)
                    if websailor_result:
                        massive_data['busca_massiva']['alibaba_websailor_results'].append(websailor_result)
                        massive_data['metadata']['apis_used'].append('alibaba_websailor')
                        logger.info(f"‚úÖ ALIBABA WebSailor: dados coletados")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è ALIBABA WebSailor falhou: {e}")

                # REAL SEARCH ORCHESTRATOR J√Å FOI EXECUTADO NO WORKFLOW - EVITAR LOOP
                # Apenas registra que os dados j√° foram coletados
                # massive_data['metadata']['apis_used'].append('real_search_orchestrator_already_executed') # Removido para evitar polui√ß√£o de metadados
                logger.info(f"‚úÖ Real Search Orchestrator: dados j√° coletados no workflow principal (se aplic√°vel)")

                # Verificar tamanho atual
                current_json = json.dumps(massive_data, ensure_ascii=False, indent=2)
                current_size = len(current_json.encode('utf-8'))

                logger.info(f"üìä Tamanho atual: {current_size/1024:.1f}KB / {self.min_size_kb}KB")

                # Pequena pausa entre buscas
                await asyncio.sleep(1)

            # Finalizar dados
            massive_data['timestamp_fim'] = datetime.now().isoformat()
            massive_data['metadata']['total_searches'] = search_count
            massive_data['metadata']['size_kb'] = current_size / 1024
            massive_data['metadata']['apis_used'] = list(set(massive_data['metadata']['apis_used']))

            # CONSOLIDA√á√ÉO: Coleta todos os dados salvos para arquivo √∫nico
            logger.info("üîÑ Consolidando todos os dados salvos...")
            massive_data = self._consolidate_all_saved_data(massive_data, session_id)

            # Salva resultado final unificado
            save_result = self.auto_save_manager.save_massive_search_result(massive_data, produto)

            if save_result.get('success'):
                logger.info(f"‚úÖ Resultado massivo CONSOLIDADO salvo: {save_result['filename']} ({save_result['size_kb']:.1f}KB)")
                return massive_data # Retorna o massive_data consolidado
            else:
                logger.error(f"‚ùå Erro ao salvar resultado massivo: {save_result.get('error')}")
                return massive_data

        except Exception as e:
            logger.error(f"‚ùå Erro na busca massiva: {e}")
            return {
                'success': False,
                'error': str(e),
                'file_path': None
            }

    def _generate_search_queries(self, produto: str, publico_alvo: str) -> List[str]:
        """Gera queries de busca massiva"""
        base_queries = [
            f"{produto} {publico_alvo}",
            f"{produto} marketing",
            f"{produto} vendas",
            f"{produto} estrat√©gia",
            f"{produto} p√∫blico alvo",
            f"{produto} mercado",
            f"{produto} tend√™ncias",
            f"{produto} concorrentes",
            f"{produto} an√°lise",
            f"{produto} insights",
            f"{produto} campanhas",
            f"{produto} convers√£o",
            f"{produto} engajamento",
            f"{produto} redes sociais",
            f"{produto} influenciadores",
            f"{produto} viral",
            f"{produto} sucesso",
            f"{produto} cases",
            f"{produto} resultados",
            f"{produto} ROI"
        ]

        # Adicionar varia√ß√µes com p√∫blico-alvo
        publico_queries = [
            f"{publico_alvo} {produto}",
            f"{publico_alvo} interesse {produto}",
            f"{publico_alvo} compra {produto}",
            f"{publico_alvo} busca {produto}",
            f"{publico_alvo} precisa {produto}"
        ]

        # Adicionar queries expandidas para garantir volume
        expanded_queries = [
            f"como vender {produto}",
            f"melhor {produto}",
            f"onde comprar {produto}",
            f"pre√ßo {produto}",
            f"avalia√ß√£o {produto}",
            f"review {produto}",
            f"opini√£o {produto}",
            f"teste {produto}",
            f"compara√ß√£o {produto}",
            f"alternativa {produto}",
            f"{produto} 2024",
            f"{produto} tend√™ncia",
            f"{produto} futuro",
            f"{produto} inova√ß√£o",
            f"{produto} tecnologia"
        ]

        return list(set(base_queries + publico_queries + expanded_queries)) # Remove duplicatas

    async def _search_alibaba_websailor(self, query: str, session_id: str) -> Optional[Dict[str, Any]]:
        """Busca usando ALIBABA WebSailor com fallback simplificado"""
        try:
            logger.info(f"üåê Tentando ALIBABA WebSailor para: {query}")

            # Tenta usar o websailor se dispon√≠vel
            if self.websailor and hasattr(self.websailor, 'navigate_and_research_deep'):
                try:
                    navigation_result = await asyncio.wait_for(
                        self.websailor.navigate_and_research_deep(
                            query=query,
                            context={'session_id': session_id},
                            max_pages=3,
                            depth_levels=1,
                            session_id=session_id
                        ),
                        timeout=30.0
                    )
                    
                    if navigation_result:
                        logger.info(f"‚úÖ ALIBABA WebSailor executado com sucesso")
                        return {
                            'query': query,
                            'api': 'alibaba_websailor',
                            'timestamp': datetime.now().isoformat(),
                            'navigation_data': navigation_result,
                            'source': 'ALIBABA_WEBSAILOR_SUCCESS',
                            'success': True
                        }
                
                except asyncio.TimeoutError:
                    logger.warning("‚ö†Ô∏è Timeout no Alibaba WebSailor")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erro no Alibaba WebSailor: {e}")
            
            # FALLBACK: Busca simples usando APIs dispon√≠veis
            logger.info(f"üîÑ Usando fallback para Alibaba WebSailor")
            
            # Tenta usar Jina como fallback
            jina_key = os.getenv('JINA_API_KEY')
            if jina_key:
                try:
                    import aiohttp
                    search_url = f"https://www.google.com/search?q={query.replace(' ', '+')}"
                    jina_url = f"https://r.jina.ai/{search_url}"
                    
                    headers = {
                        'Authorization': f'Bearer {jina_key}',
                        'Accept': 'text/plain'
                    }
                    
                    timeout = aiohttp.ClientTimeout(total=15)
                    async with aiohttp.ClientSession(timeout=timeout) as session:
                        async with session.get(jina_url, headers=headers) as response:
                            if response.status == 200:
                                content = await response.text()
                                
                                return {
                                    'query': query,
                                    'api': 'alibaba_websailor_fallback',
                                    'timestamp': datetime.now().isoformat(),
                                    'fallback_data': content[:2000],
                                    'source': 'ALIBABA_WEBSAILOR_FALLBACK',
                                    'success': True
                                }
                                
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Fallback Jina falhou: {e}")
            
            return {
                'query': query,
                'api': 'alibaba_websailor',
                'timestamp': datetime.now().isoformat(),
                'error': 'N√£o foi poss√≠vel executar busca',
                'source': 'ALIBABA_WEBSAILOR_FAILED',
                'success': False
            }
            
        except Exception as e:
            logger.error(f"‚ùå ALIBABA WebSailor falhou completamente: {e}")
            return {
                'query': query,
                'api': 'alibaba_websailor',
                'timestamp': datetime.now().isoformat(),
                'error': str(e),
                'source': 'ALIBABA_WEBSAILOR_ERROR',
                'success': False
            }

    async def _search_real_orchestrator(self, query: str, session_id: str) -> Optional[Dict[str, Any]]:
        """Busca usando Real Search Orchestrator - SISTEMA PRINCIPAL"""
        try:
            logger.info(f"üéØ Real Search Orchestrator executando busca: {query}")

            # Usa o m√©todo CORRETO que existe no RealSearchOrchestrator
            result = await self.real_search.execute_massive_real_search(
                query=query,
                context={'session_id': session_id, 'produto': query},
                session_id=session_id
            )

            # Extrai dados v√°lidos do resultado
            if result and isinstance(result, dict):
                return {
                    'query': query,
                    'api': 'real_search_orchestrator',
                    'timestamp': datetime.now().isoformat(),
                    'data': result,
                    'web_results_count': len(result.get('web_results', [])),
                    'social_results_count': len(result.get('social_results', [])),
                    'youtube_results_count': len(result.get('youtube_results', [])),
                    'source': 'REAL_SEARCH_ORCHESTRATOR_PRINCIPAL'
                }
            else:
                logger.warning(f"‚ö†Ô∏è Real Search Orchestrator retornou dados inv√°lidos")
                return None

        except Exception as e:
            logger.error(f"‚ùå Real Search Orchestrator falhou: {e}")
            return None

    def _calculate_final_size(self, massive_data: Dict[str, Any]) -> float:
        """Calcula tamanho final em KB"""
        try:
            json_str = json.dumps(massive_data, ensure_ascii=False)
            return len(json_str.encode('utf-8')) / 1024
        except Exception as e:
            logger.error(f"‚ùå Erro ao calcular tamanho: {e}")
            return 0.0

    def _consolidate_all_saved_data(self, massive_data: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """
        CONSOLIDA√á√ÉO TEXTUAL: Coleta APENAS texto para an√°lise da IA
        Remove imagens e mant√©m apenas dados textuais essenciais
        """
        try:
            logger.info("üìù Iniciando consolida√ß√£o TEXTUAL para an√°lise da IA...")

            dados_consolidados = {
                'textos_pesquisa_web': [],
                'textos_redes_sociais': [],
                'insights_extraidos': [],
                'trechos_navegacao': [],
                'metadados_fontes': [],
                'etapas_extracao': [],
                'modulos_analises': [],
                'jsons_gigantes': [],
                'resultados_virais': [],
                'metadata_consolidacao': {
                    'timestamp_consolidacao': datetime.now().isoformat(),
                    'session_id': session_id,
                    'total_textos_processados': 0,
                    'fontes_unicas': 0,
                    'caracteres_totais': 0,
                    'categorias_encontradas': [],
                    'finalidade': 'ALIMENTAR_IA_SEGUNDA_ETAPA'
                }
            }

            textos_processados = 0
            caracteres_totais = 0
            urls_unicas = set()

            def _extract_and_add_text(data_item: Any, source_type: str, url: Optional[str] = None):
                nonlocal textos_processados, caracteres_totais, urls_unicas
                if isinstance(data_item, dict):
                    for key, value in data_item.items():
                        if isinstance(value, str) and len(value) > 50:
                            dados_consolidados['trechos_navegacao'].append({
                                'fonte': f"{source_type}_{key}",
                                'texto': value,
                                'caracteres': len(value)
                            })
                            caracteres_totais += len(value)
                            textos_processados += 1
                elif isinstance(data_item, list):
                    for item in data_item:
                        if isinstance(item, str) and len(item) > 50:
                            dados_consolidados['trechos_navegacao'].append({
                                'fonte': source_type,
                                'texto': item,
                                'caracteres': len(item)
                            })
                            caracteres_totais += len(item)
                            textos_processados += 1
                if url and url != 'N/A':
                    urls_unicas.add(url)

            # 1. COLETA TEXTOS DA PESQUISA WEB (ALIBABA WebSailor)
            try:
                if 'alibaba_websailor_results' in massive_data.get('busca_massiva', {}):
                    for result in massive_data['busca_massiva']['alibaba_websailor_results']:
                        if result and isinstance(result, dict):
                            nav_data = result.get('navigation_data', {})
                            if nav_data and isinstance(nav_data, dict):
                                conteudo = nav_data.get('conteudo_consolidado', {})
                                
                                # Textos principais
                                for texto in conteudo.get('textos_principais', []):
                                    dados_consolidados['textos_pesquisa_web'].append({
                                        'fonte': 'websailor_navegacao',
                                        'url': result.get('query', 'N/A'),
                                        'texto': str(texto),
                                        'caracteres': len(str(texto))
                                    })
                                    caracteres_totais += len(str(texto))
                                    textos_processados += 1
                                    if result.get('query') and result.get('query') != 'N/A':
                                        urls_unicas.add(result['query'])
                                
                                # Insights extra√≠dos
                                for insight in conteudo.get('insights_principais', []):
                                    dados_consolidados['insights_extraidos'].append({
                                        'fonte': 'websailor_insights',
                                        'insight': str(insight),
                                        'caracteres': len(str(insight))
                                    })
                                    caracteres_totais += len(str(insight))
                
                logger.info(f"‚úÖ Coletados {textos_processados} textos da pesquisa web (WebSailor)")
            except Exception as e:
                logger.error(f"‚ùå Erro ao coletar textos web (WebSailor): {e}")

            # 2. COLETA DADOS SALVOS EM ETAPAS ANTERIORES (do auto_save_manager)
            try:
                session_data = self.auto_save_manager.recuperar_etapa(session_id)
                if session_data and isinstance(session_data, dict):
                    extraction_steps = session_data.get('extraction_steps', [])
                    for step in extraction_steps:
                        try:
                            step_data = step.get('dados', {})
                            _extract_and_add_text(step_data, f"etapa_extracao_{step.get('nome', 'step')}")
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è Erro ao processar step do auto_save_manager: {e}")
                logger.info(f"‚úÖ Processados steps salvos do auto_save_manager")
            except Exception as e:
                logger.error(f"‚ùå Erro ao coletar etapas salvas do auto_save_manager: {e}")

            # 3. COLETA DADOS DE DIRET√ìRIOS ESPEC√çFICOS (relatorios_intermediarios, modulos_analises, jsons_gigantes, resultados_virais, pesquisa_web)
            data_categories = {
                'relatorios_intermediarios': 'etapas_extracao',
                'modulos_analises': 'modulos_analises',
                'jsons_gigantes': 'jsons_gigantes',
                'resultados_virais': 'resultados_virais',
                'pesquisa_web': 'trechos_pesquisa_web'
            }

            for category_dir, target_list_name in data_categories.items():
                try:
                    base_path = os.path.join(self.data_dir, category_dir)
                    if os.path.exists(base_path):
                        session_path = os.path.join(base_path, session_id)
                        if os.path.exists(session_path):
                            for arquivo in os.listdir(session_path):
                                if arquivo.endswith('.json'):
                                    arquivo_path = os.path.join(session_path, arquivo)
                                    try:
                                        with open(arquivo_path, 'r', encoding='utf-8') as f:
                                            dados_arquivo = json.load(f)
                                            
                                            # Adiciona ao massive_data['dados_consolidados_texto'] diretamente
                                            dados_consolidados[target_list_name].append({
                                                'arquivo_origem': arquivo,
                                                'tipo': category_dir,
                                                'dados': dados_arquivo
                                            })
                                            _extract_and_add_text(dados_arquivo, f"{category_dir}_file")

                                    except Exception as e:
                                        logger.warning(f"‚ö†Ô∏è Erro ao ler {arquivo_path} na categoria {category_dir}: {e}")
                    logger.info(f"‚úÖ Coletados dados da categoria: {category_dir}")
                except Exception as e:
                    logger.error(f"‚ùå Erro ao coletar dados da categoria {category_dir}: {e}")

            # 4. ADICIONA METADADOS PARA A IA
            dados_consolidados['metadata_consolidacao'].update({
                'total_textos_processados': textos_processados,
                'caracteres_totais': caracteres_totais,
                'fontes_unicas': len(urls_unicas),
                'tamanho_kb': caracteres_totais / 1024,
                'urls_fonte': list(urls_unicas)[:10],  # M√°ximo 10 URLs para contexto
                'resumo_coleta': f"{textos_processados} textos, {caracteres_totais:,} chars, {len(urls_unicas)} fontes"
            })
            
            logger.info(f"üìä CONSOLIDA√á√ÉO TEXTUAL FINAL: {textos_processados} textos, {caracteres_totais:,} caracteres")
            
            # Adiciona dados consolidados ao massive_data (APENAS TEXTO)
            massive_data['dados_consolidados_texto'] = dados_consolidados

            logger.info(f"‚úÖ CONSOLIDA√á√ÉO TEXTUAL CONCLU√çDA: {textos_processados} textos processados")
            logger.info(f"üìù Total: {caracteres_totais:,} caracteres para an√°lise da IA")

            return massive_data

        except Exception as e:
            logger.error(f"‚ùå Erro na consolida√ß√£o de dados: {e}")
            # Retorna dados originais se falhar
            return massive_data


# Inst√¢ncia global
massive_search_engine = MassiveSearchEngine()

